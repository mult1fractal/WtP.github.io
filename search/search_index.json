{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"CaSe-Group What the Phage: A scalable workflow for the identification and analysis of phage sequences M. Marquet, M. H\u00f6lzer, M. W. Pletz, A. Viehweger, O. Makarewicz, R. Ehricht, C. Brandt doi: https://doi.org/10.1101/2020.07.24.219899 System Requirements Components minimum recomended OS Linux like Linux like Cores 4 8 Memory 4 GB RAM 8 GB RAM Storage 50 GB available space 128-256 GB available space Why so much space? -.- What the Phage Phages are among the most abundant and diverse biological entities on earth. Identification from sequence data is a crucial first step to understand their impact on the environment. A variety of bacteriophage identification tools have been developed over the years. They differ in algorithmic approach, results and ease of use. We, therefore, developed \u201cWhat the Phage\u201d (WtP), an easy-to-use and parallel multitool approach for phage identification combined with an annotation and classification downstream strategy, thus, supporting the user\u2019s decision-making process when the phage identification tools are not in agreement to each other. WtP is reproducible and scales to thousands of datasets through the use of a workflow manager (Nextflow). Under the hood Figure 3: This plot shows a simplified dag-chart of WtP for better understanding of what's going on behind the curtain Included tools Identification Toolname/Gitlink Reference MARVEL MARVEL, a Tool for Prediction of Bacteriophage Sequences in Metagenomic Bins VirFinder VirFinder: R package for identifying viral sequences from metagenomic data using sequence signatures PPR-Meta PPR-Meta: a tool for identifying phages and plasmids from metagenomic fragments using deep learning VirSorter VirSorter: mining viral signal from microbial genomic data MetaPhinder MetaPhinder\u2014Identifying Bacteriophage Sequences in Metagenomic Data Sets DeepVirFinder Identifying viruses from metagenomic data by deep learning Sourmash sourmash: a library for MinHash sketching of DNA VIBRANT Automated recovery, annotation and curation of microbial viruses, and evaluation of virome function from genomic sequences VirNet Deep attention model for viral reads identification Phigaro Phigaro: high throughput prophage sequence annotation Virsorter2 beta WIP Seeker Seeker: alignment-free identification of bacteriophage genomes by deep learning Annotation & classification Toolname/Git Reference prodigal Prodigal: prokaryotic gene recognition and translation initiation site identification hmmer nhmmer: DNA homology search with profile HMMs chromomap CheckV CheckV: assessing the quality of metagenome-assembled viral genomes Other tools Toolname/Git Reference samtools The Sequence Alignment/Map format and SAMtools seqkit SeqKit: A Cross-Platform and Ultrafast Toolkit for FASTA/Q File Manipulation UpSetR UpSetR: an R package for the visualization of intersecting sets and their properties .","title":"Home"},{"location":"index.html#system-requirements","text":"Components minimum recomended OS Linux like Linux like Cores 4 8 Memory 4 GB RAM 8 GB RAM Storage 50 GB available space 128-256 GB available space Why so much space? -.-","title":"System Requirements"},{"location":"index.html#what-the-phage","text":"Phages are among the most abundant and diverse biological entities on earth. Identification from sequence data is a crucial first step to understand their impact on the environment. A variety of bacteriophage identification tools have been developed over the years. They differ in algorithmic approach, results and ease of use. We, therefore, developed \u201cWhat the Phage\u201d (WtP), an easy-to-use and parallel multitool approach for phage identification combined with an annotation and classification downstream strategy, thus, supporting the user\u2019s decision-making process when the phage identification tools are not in agreement to each other. WtP is reproducible and scales to thousands of datasets through the use of a workflow manager (Nextflow).","title":"What the Phage"},{"location":"index.html#under-the-hood","text":"Figure 3: This plot shows a simplified dag-chart of WtP for better understanding of what's going on behind the curtain","title":"Under the hood"},{"location":"index.html#included-tools","text":"","title":"Included tools"},{"location":"index.html#identification","text":"Toolname/Gitlink Reference MARVEL MARVEL, a Tool for Prediction of Bacteriophage Sequences in Metagenomic Bins VirFinder VirFinder: R package for identifying viral sequences from metagenomic data using sequence signatures PPR-Meta PPR-Meta: a tool for identifying phages and plasmids from metagenomic fragments using deep learning VirSorter VirSorter: mining viral signal from microbial genomic data MetaPhinder MetaPhinder\u2014Identifying Bacteriophage Sequences in Metagenomic Data Sets DeepVirFinder Identifying viruses from metagenomic data by deep learning Sourmash sourmash: a library for MinHash sketching of DNA VIBRANT Automated recovery, annotation and curation of microbial viruses, and evaluation of virome function from genomic sequences VirNet Deep attention model for viral reads identification Phigaro Phigaro: high throughput prophage sequence annotation Virsorter2 beta WIP Seeker Seeker: alignment-free identification of bacteriophage genomes by deep learning","title":"Identification"},{"location":"index.html#annotation-classification","text":"Toolname/Git Reference prodigal Prodigal: prokaryotic gene recognition and translation initiation site identification hmmer nhmmer: DNA homology search with profile HMMs chromomap CheckV CheckV: assessing the quality of metagenome-assembled viral genomes","title":"Annotation &amp; classification"},{"location":"index.html#other-tools","text":"Toolname/Git Reference samtools The Sequence Alignment/Map format and SAMtools seqkit SeqKit: A Cross-Platform and Ultrafast Toolkit for FASTA/Q File Manipulation UpSetR UpSetR: an R package for the visualization of intersecting sets and their properties .","title":"Other tools"},{"location":"troubleshooting.html","text":"FAQ In this section I (mult1fractal) have some troubleshooting advice with problems I faced when I started with bioinformatics and testing with WtP Problems with storage while running WtP WtP produces temporary data. Depending on your input file, this temporary data can take up a lot of GB of storage space after several WtP runs By default, all temporary data files are stored in /tmp/nextflow-phage-$USER . i.e. When you restart, the temporary data files will be removed. For users who run WtP on a cluster and can't restart it, we have the --work-dir flag. This makes it possible to change the storage location for the temporary workflow files. With the flag --work-dir work , a folder with the name work will be created in your current working dir. All of WtP temporary workflow files will be stored in this directory. With sudo rm -r / work * they can become demanding. --work-dir /path/to/dir # defines the path where nextflow writes temporary files, default: '/tmp/nextflow-phage-$USER' Installing WTP in a centralized way for multi-users Run WtP on a cluster environment: Let the Users run WtP from their individual accounts (e.g., via ssh connection) create special locations (shared locations) where to store the singularity images, docker images, the databases and the cache let each run of WtP (from a user) use the shared locations but the usage provided forces the users to specify params --workdir, --databases and --cache...... the shared locations should be transparent to the users .. Quick Solution: \"install\" WtP via git clone --branch v1.0.0 https://github.com/replikation/What_the_Phage.git then change the nextflow config and let the user use this \"git\" (the version would then be fixed to the git clone) e.g.: ./phage.nf \\ --fasta /path/to/file.fa \\ # provide a fasta-file as input --cores 8 \\ # number of cores you want to use -profile local,docker # choose the environment:local and docker Error ignored: some tools can fail during the identification process e.g. if a tool can't process a fasta-inputfile (sometimes MARVEL with really large multi fasta files) then you will get an error-message like this : [34/31d18f] NOTE: Process `identify_fasta_MSF:upsetr_plot (1)` terminated with an error exit status (1) -- Error is ignored even if an error occures WtP continues its work, but won't include the results of the failed process Chromomap issues: terminated with an error exit status (1) In the Annotation process of WtP, wird eine Genkarte mit den annotierten/gefundenen Genen erstellt Bei der erstellung der Genkarte kann es manchmal zu folgendem Error-code kommen: [4e/57d82d] NOTE: Process 'phage_annotation_MSF:chromomap (1)' terminated with an error exit status (1) -- Execution is retried (1) The workflow works as intended. We included a few \"failsafe\" processes for chromomap, so if one plotting approach fails it retries another approach to render it, thus you get the fail but the retry will work. In the end you will get the result Singularity image problems Sometimes the singularity runs fail because some singularity images are failing in their build process: you can retry the the execution command or you can run the Pre-download for Offline-mode and start WtP with the 'preloaded' images Download Databases manually usually there is no problem with database download. in case there is a problem you can download them here","title":"Troubleshooting"},{"location":"troubleshooting.html#faq","text":"In this section I (mult1fractal) have some troubleshooting advice with problems I faced when I started with bioinformatics and testing with WtP","title":"FAQ"},{"location":"troubleshooting.html#problems-with-storage-while-running-wtp","text":"WtP produces temporary data. Depending on your input file, this temporary data can take up a lot of GB of storage space after several WtP runs By default, all temporary data files are stored in /tmp/nextflow-phage-$USER . i.e. When you restart, the temporary data files will be removed. For users who run WtP on a cluster and can't restart it, we have the --work-dir flag. This makes it possible to change the storage location for the temporary workflow files. With the flag --work-dir work , a folder with the name work will be created in your current working dir. All of WtP temporary workflow files will be stored in this directory. With sudo rm -r / work * they can become demanding. --work-dir /path/to/dir # defines the path where nextflow writes temporary files, default: '/tmp/nextflow-phage-$USER'","title":"Problems with storage while running WtP"},{"location":"troubleshooting.html#installing-wtp-in-a-centralized-way-for-multi-users","text":"Run WtP on a cluster environment: Let the Users run WtP from their individual accounts (e.g., via ssh connection) create special locations (shared locations) where to store the singularity images, docker images, the databases and the cache let each run of WtP (from a user) use the shared locations but the usage provided forces the users to specify params --workdir, --databases and --cache...... the shared locations should be transparent to the users .. Quick Solution: \"install\" WtP via git clone --branch v1.0.0 https://github.com/replikation/What_the_Phage.git then change the nextflow config and let the user use this \"git\" (the version would then be fixed to the git clone) e.g.: ./phage.nf \\ --fasta /path/to/file.fa \\ # provide a fasta-file as input --cores 8 \\ # number of cores you want to use -profile local,docker # choose the environment:local and docker","title":"Installing WTP in a centralized way for multi-users"},{"location":"troubleshooting.html#error-ignored","text":"some tools can fail during the identification process e.g. if a tool can't process a fasta-inputfile (sometimes MARVEL with really large multi fasta files) then you will get an error-message like this : [34/31d18f] NOTE: Process `identify_fasta_MSF:upsetr_plot (1)` terminated with an error exit status (1) -- Error is ignored even if an error occures WtP continues its work, but won't include the results of the failed process","title":"Error ignored:"},{"location":"troubleshooting.html#chromomap-issues-terminated-with-an-error-exit-status-1","text":"In the Annotation process of WtP, wird eine Genkarte mit den annotierten/gefundenen Genen erstellt Bei der erstellung der Genkarte kann es manchmal zu folgendem Error-code kommen: [4e/57d82d] NOTE: Process 'phage_annotation_MSF:chromomap (1)' terminated with an error exit status (1) -- Execution is retried (1) The workflow works as intended. We included a few \"failsafe\" processes for chromomap, so if one plotting approach fails it retries another approach to render it, thus you get the fail but the retry will work. In the end you will get the result","title":"Chromomap issues: terminated with an error exit status (1)"},{"location":"troubleshooting.html#singularity-image-problems","text":"Sometimes the singularity runs fail because some singularity images are failing in their build process: you can retry the the execution command or you can run the Pre-download for Offline-mode and start WtP with the 'preloaded' images","title":"Singularity image problems"},{"location":"troubleshooting.html#download-databases-manually","text":"usually there is no problem with database download. in case there is a problem you can download them here","title":"Download Databases manually"},{"location":"Workflow-execution/Overview.html","text":"Command overview The basic command nextflow run \\ # calling the workflow replikation/What_the_Phage \\ # WtP Git-Repo --fasta /path/to/file.fa \\ # provide a fasta-file as input --cores 8 \\ # number of cores you want to use -profile local,docker # choose the environment:local and docker -r v1.0.0 # WtP release version Flag overview Mandatory Flag simple explanation --fasta path/to/phage-assembly.fa or '/path/to/*.fa' -profile local,docker or local,singularity or lsf,docker -r v1.0.0 --fastq alternatively to --fasta (still in development phase)/path/to/phage-read.fastq or '/path/to/*.fastq' Options Flag simple explanation --help will show you this page in your terminal --filter e.g. 1500 bp (sequences below 1500 bp won't be analyzed) --cores e.g. 10 --setup pre-downloads all you need to run WtP Pathing Flag simple explanation --workdir /path/to/dir --database /path/to/dir --cachedir /path/to/dir --output e.g. results Workflow control Flag simple explanation --dv deactivates deepvirfinder --ma deactivates marvel --mp deactivates metaphinder --pp deactivates PPRmeta --sm deactivates sourmash --vb deactivates vibrant --vf deactivates virfinder --vn deactivates virnet --vs deactivates virsorter --ph deactivates phigaro --vs2 deactivates virsorter2 --identify only phage identification, skips analysis --annotate only annotation, skips phage identification","title":"Overview"},{"location":"Workflow-execution/Overview.html#command-overview","text":"","title":"Command overview"},{"location":"Workflow-execution/Overview.html#the-basic-command","text":"nextflow run \\ # calling the workflow replikation/What_the_Phage \\ # WtP Git-Repo --fasta /path/to/file.fa \\ # provide a fasta-file as input --cores 8 \\ # number of cores you want to use -profile local,docker # choose the environment:local and docker -r v1.0.0 # WtP release version","title":"The basic command"},{"location":"Workflow-execution/Overview.html#flag-overview","text":"","title":"Flag overview"},{"location":"Workflow-execution/Overview.html#mandatory","text":"Flag simple explanation --fasta path/to/phage-assembly.fa or '/path/to/*.fa' -profile local,docker or local,singularity or lsf,docker -r v1.0.0 --fastq alternatively to --fasta (still in development phase)/path/to/phage-read.fastq or '/path/to/*.fastq'","title":"Mandatory"},{"location":"Workflow-execution/Overview.html#options","text":"Flag simple explanation --help will show you this page in your terminal --filter e.g. 1500 bp (sequences below 1500 bp won't be analyzed) --cores e.g. 10 --setup pre-downloads all you need to run WtP","title":"Options"},{"location":"Workflow-execution/Overview.html#pathing","text":"Flag simple explanation --workdir /path/to/dir --database /path/to/dir --cachedir /path/to/dir --output e.g. results","title":"Pathing"},{"location":"Workflow-execution/Overview.html#workflow-control","text":"Flag simple explanation --dv deactivates deepvirfinder --ma deactivates marvel --mp deactivates metaphinder --pp deactivates PPRmeta --sm deactivates sourmash --vb deactivates vibrant --vf deactivates virfinder --vn deactivates virnet --vs deactivates virsorter --ph deactivates phigaro --vs2 deactivates virsorter2 --identify only phage identification, skips analysis --annotate only annotation, skips phage identification","title":"Workflow control"},{"location":"Workflow-execution/commands.html","text":"Detailed Flag explanation Inputs Input examples: wildcards need single quotes around the path ( ' ) --fasta /path/to/phage-assembly.fa # path to your fasta-file --fasta '/path/to/*.fa' # path to all .fa files in a dir --fastq /path/to/phage-read.fastq # path to your fastq-file --fastq '/path/to/*.fastq' # path to all .fastq files in a dir the fastq input is currently experimental Profiles Choose the environment: local , slurm , lsf or ebi Choose the engine: docker or singularity examples: -profile local,docker -profile local,singularity -profile lsf,docker Release candidate A release candidate is a released version of WtP which ensures proper functionality version control ensures reproducibility as each tools version is also \"locked\" within the release candidate databases have no automatic version control (they are downloaded from the source) if you need version control for databases, just make a copy of the database dir after download you can specify the database dir via the --database flag (see below) WtP only downloads a database if it's missing, it is not \"auto-updating\" them add this flag to your command and a specific release is used instead -r v1.0.0 Data handling WtP handles everything by default If you need to change paths use the following commands It's useful to specify --workdir to your current working dir if /tmp (default) has limited space -work-dir /path/to/dir # defines the path where nextflow writes temporary files, default: '/tmp/nextflow-phage-$USER' --databases /path/to/dir # specify download location of databases, default './nextflow-autodownload-databases' --cachedir /path/to/dir # defines the path where singularity images are cached, default './singularity-images' --output results # path of the outdir, default './results' Pre-download for Offline-mode --setup skips analysis and just downloads all databases and containers Needs roughly 30 GB storage for databases, excluding programs nextflow run replikation/What_the_Phage --setup -r v1.0.0 -profile singularity,local you can change the database download location via (--databases) make sure that you specify the database location when executing WtP, if you change the default path singularity images sometimes fail during building, just try to re-execute --setup WtP attempts to build images up to 3 times, image building is individually skipped if present","title":"Commands"},{"location":"Workflow-execution/commands.html#detailed-flag-explanation","text":"","title":"Detailed Flag explanation"},{"location":"Workflow-execution/commands.html#inputs","text":"Input examples: wildcards need single quotes around the path ( ' ) --fasta /path/to/phage-assembly.fa # path to your fasta-file --fasta '/path/to/*.fa' # path to all .fa files in a dir --fastq /path/to/phage-read.fastq # path to your fastq-file --fastq '/path/to/*.fastq' # path to all .fastq files in a dir the fastq input is currently experimental","title":"Inputs"},{"location":"Workflow-execution/commands.html#profiles","text":"Choose the environment: local , slurm , lsf or ebi Choose the engine: docker or singularity examples: -profile local,docker -profile local,singularity -profile lsf,docker","title":"Profiles"},{"location":"Workflow-execution/commands.html#release-candidate","text":"A release candidate is a released version of WtP which ensures proper functionality version control ensures reproducibility as each tools version is also \"locked\" within the release candidate databases have no automatic version control (they are downloaded from the source) if you need version control for databases, just make a copy of the database dir after download you can specify the database dir via the --database flag (see below) WtP only downloads a database if it's missing, it is not \"auto-updating\" them add this flag to your command and a specific release is used instead -r v1.0.0","title":"Release candidate"},{"location":"Workflow-execution/commands.html#data-handling","text":"WtP handles everything by default If you need to change paths use the following commands It's useful to specify --workdir to your current working dir if /tmp (default) has limited space -work-dir /path/to/dir # defines the path where nextflow writes temporary files, default: '/tmp/nextflow-phage-$USER' --databases /path/to/dir # specify download location of databases, default './nextflow-autodownload-databases' --cachedir /path/to/dir # defines the path where singularity images are cached, default './singularity-images' --output results # path of the outdir, default './results'","title":"Data handling"},{"location":"Workflow-execution/commands.html#pre-download-for-offline-mode","text":"--setup skips analysis and just downloads all databases and containers Needs roughly 30 GB storage for databases, excluding programs nextflow run replikation/What_the_Phage --setup -r v1.0.0 -profile singularity,local you can change the database download location via (--databases) make sure that you specify the database location when executing WtP, if you change the default path singularity images sometimes fail during building, just try to re-execute --setup WtP attempts to build images up to 3 times, image building is individually skipped if present","title":"Pre-download for Offline-mode"},{"location":"Workflow-execution/processes.html","text":"WtP terminal execution: In this section we want to explain what you see in the Terminal when you execute WtP: I'm currently writing on this part Let's do a Testrun Open your terminal, move to a directory where you want to execute WtP and type/copy the following command nextflow run phage.nf -profile smalltest,local,docker --work-dir work --cores 16 A lot of stuff will pop up but we will quide you through it: N E X T F L O W ~ version 20.07.1 Launching `phage.nf` [admiring_pesquet] - revision: 1b76c726d3 _____ _____ ____ ____ ___ ___ __ __ _ _ __ _______________________ / \\ / \\__ ___/\\______ \\ \\ \\/\\/ / | | | ___/ \\ / | | | | \\__/\\ / |____| |____| \\/ _____ _____ ____ ____ ___ ___ __ __ _ _ Profile: smalltest,local,docker Current User: mike Nextflow-version: 20.07.1 WtP intended for Nextflow-version: 20.01.0 Starting time: 24-07-2020 15:18 UTC Workdir location [--workdir]: /home/mike/bioinformatics/What_the_Phage/cores=16 Output location [--output]: results Database location [--databases]: nextflow-autodownload-databases CPUs to use: 8, maximal CPUs to use: 20 this is the first thing that will pop up and will give you some basic workflow information: eg where output is stored or how many cores you are using for computing An overview about all the WtP processes will show up and looks like this: [6b/79564a] process > identify_fasta_MSF:fasta_validation_wf:input_suffix_check (1) [100%] 1 of 1 \u2714 [c7/d34016] process > identify_fasta_MSF:fasta_validation_wf:seqkit (1) [100%] 1 of 1 \u2714 [- ] process > identify_fasta_MSF:virsorter_wf:virsorter [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:virsorter_wf:filter_virsorter - [- ] process > identify_fasta_MSF:virsorter_wf:virsorter_collect_data - [ff/f698ec] process > identify_fasta_MSF:virsorter2_wf:virsorter2 (1) [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:virsorter2_wf:filter_virsorter2 - * * * executor > local (9) [skipped ] process > phage_references:download_references [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > phage_blast_DB:phage_references_blastDB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > ppr_dependecies:ppr_download_dependencies [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > vibrant_database:vibrant_download_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > virsorter_database:virsorter_download_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > pvog_database:pvog_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > vogtable_database:vogtable_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > vog_database:vog_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > rvdb_database:rvdb_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > checkV_database:download_checkV_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > sourmash_database:sourmash_download_DB [100%] 1 of 1, stored: 1 \u2714 [6b/79564a] process > identify_fasta_MSF:fasta_validation_wf:input_suffix_check (1) [100%] 1 of 1 \u2714 [c7/d34016] process > identify_fasta_MSF:fasta_validation_wf:seqkit (1) [100%] 1 of 1 \u2714 [- ] process > identify_fasta_MSF:virsorter_wf:virsorter [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:virsorter_wf:filter_virsorter - [- ] process > identify_fasta_MSF:virsorter_wf:virsorter_collect_data - [ff/f698ec] process > identify_fasta_MSF:virsorter2_wf:virsorter2 (1) [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:virsorter2_wf:filter_virsorter2 - [- ] process > identify_fasta_MSF:virsorter2_wf:virsorter2_collect_data - * * * [fc/e0fcc7] process > identify_fasta_MSF:pprmeta_wf:pprmeta (1) [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:pprmeta_wf:filter_PPRmeta - [- ] process > identify_fasta_MSF:pprmeta_wf:pprmeta_collect_data - [- ] process > identify_fasta_MSF:vibrant_wf:vibrant [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:vibrant_wf:filter_vibrant - [- ] process > identify_fasta_MSF:vibrant_wf:vibrant_collect_data - [- ] process > identify_fasta_MSF:vibrant_virome_wf:vibrant_virome [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:vibrant_virome_wf:filter_vibrant_virome - [- ] process > identify_fasta_MSF:vibrant_virome_wf:vibrant_virome_collect_data - [08/50ce71] process > identify_fasta_MSF:virnet_wf:normalize_contig_size (1) [100%] 1 of 1 \u2714 [- ] process > identify_fasta_MSF:virnet_wf:virnet [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:virnet_wf:filter_virnet - [- ] process > identify_fasta_MSF:virnet_wf:virnet_collect_data - [4e/ca5a07] process > identify_fasta_MSF:phigaro_wf:phigaro (1) [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:phigaro_wf:phigaro_collect_data - [- ] process > identify_fasta_MSF:seeker_wf:seeker [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:seeker_wf:filter_seeker - [- ] process > identify_fasta_MSF:seeker_wf:seeker_collect_data - [- ] process > identify_fasta_MSF:filter_tool_names - [- ] process > identify_fasta_MSF:r_plot - [- ] process > identify_fasta_MSF:upsetr_plot - [- ] process > identify_fasta_MSF:samtools - [- ] process > phage_annotation_MSF:prodigal - [- ] process > phage_annotation_MSF:hmmscan - [- ] process > phage_annotation_MSF:chromomap_parser - [- ] process > phage_annotation_MSF:chromomap - [- ] process > checkV_wf:checkV - [- ] process > phage_tax_classification:split_multi_fasta - [- ] process > phage_tax_classification:sourmash_for_tax - Started process [4e/ca5a07] process > identify_fasta_MSF:phigaro_wf:phigaro (1) [ 0%] 0 of 1 [4e/ca5a07] : is the process folder where the tasks of this process is started/executed identify_fasta_MSF : is one of two sub workflows (identify sequence; phage annotation) phigaro_wf: : the Phigaro identification process contains a smaller workflow with phigaro and phigaro_collect_data (raw output of Phigaro) phigaro : is the actual phage identification task [ 0%] 0 of 1 : Process has not finished yet Finished Process this is how a finished process will look like this: [6b/79564a] process > identify_fasta_MSF:fasta_validation_wf:input_suffix_check (1) [100%] 1 of 1 \u2714 [6b/79564a] : is the process folder where the tasks of this process (input_suffix_check) is executed you can access the folder: default: cd /tmp/nextflow-phage-$USER/6b/79564a and tabcompletion in our case with --work-dir : cd work/6b/79564a and tab completion you will see the files used/created in this process identify_fasta_MSF : is one of two sub workflows (identify sequence; phage annotation) input_suffix_check (1) : is the actual task (in this case: checks if the fasta file has the correct format) [100%] 1 of 1 \u2714 : tells you that the process is complete Stored process [skipped ] process > phage_blast_DB:phage_references_blastDB [100%] 1 of 1, stored: 1 \u2714 this is shown when you already e.g. downloaded the database so WtP won't execute this process again because the data is already stored Process in Queue [- ] process > identify_fasta_MSF:phigaro_wf:phigaro_collect_data - this is shows you that the process has not been started yet and is queued and waiting for execution OnComplete Done! Results are stored here --> results Thank you for using What the Phage Please cite us: https://doi.org/10.1101/2020.07.24.219899 Please also cite the other tools we use in our workflow --> results/literature Completed at: 30-Oct-2020 10:15:27 Duration : 49m 34s CPU hours : 5.2 Succeeded : 61 the onComplete message is shown when WtP is done here you find again some basic information e.g. how long was your run, where are the results and a literature-file for citing us and the tools WtP uses","title":"Testrun"},{"location":"Workflow-execution/processes.html#wtp-terminal-execution","text":"In this section we want to explain what you see in the Terminal when you execute WtP: I'm currently writing on this part","title":"WtP terminal execution:"},{"location":"Workflow-execution/processes.html#lets-do-a-testrun","text":"Open your terminal, move to a directory where you want to execute WtP and type/copy the following command nextflow run phage.nf -profile smalltest,local,docker --work-dir work --cores 16 A lot of stuff will pop up but we will quide you through it: N E X T F L O W ~ version 20.07.1 Launching `phage.nf` [admiring_pesquet] - revision: 1b76c726d3 _____ _____ ____ ____ ___ ___ __ __ _ _ __ _______________________ / \\ / \\__ ___/\\______ \\ \\ \\/\\/ / | | | ___/ \\ / | | | | \\__/\\ / |____| |____| \\/ _____ _____ ____ ____ ___ ___ __ __ _ _ Profile: smalltest,local,docker Current User: mike Nextflow-version: 20.07.1 WtP intended for Nextflow-version: 20.01.0 Starting time: 24-07-2020 15:18 UTC Workdir location [--workdir]: /home/mike/bioinformatics/What_the_Phage/cores=16 Output location [--output]: results Database location [--databases]: nextflow-autodownload-databases CPUs to use: 8, maximal CPUs to use: 20 this is the first thing that will pop up and will give you some basic workflow information: eg where output is stored or how many cores you are using for computing An overview about all the WtP processes will show up and looks like this: [6b/79564a] process > identify_fasta_MSF:fasta_validation_wf:input_suffix_check (1) [100%] 1 of 1 \u2714 [c7/d34016] process > identify_fasta_MSF:fasta_validation_wf:seqkit (1) [100%] 1 of 1 \u2714 [- ] process > identify_fasta_MSF:virsorter_wf:virsorter [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:virsorter_wf:filter_virsorter - [- ] process > identify_fasta_MSF:virsorter_wf:virsorter_collect_data - [ff/f698ec] process > identify_fasta_MSF:virsorter2_wf:virsorter2 (1) [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:virsorter2_wf:filter_virsorter2 - * * * executor > local (9) [skipped ] process > phage_references:download_references [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > phage_blast_DB:phage_references_blastDB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > ppr_dependecies:ppr_download_dependencies [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > vibrant_database:vibrant_download_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > virsorter_database:virsorter_download_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > pvog_database:pvog_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > vogtable_database:vogtable_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > vog_database:vog_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > rvdb_database:rvdb_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > checkV_database:download_checkV_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > sourmash_database:sourmash_download_DB [100%] 1 of 1, stored: 1 \u2714 [6b/79564a] process > identify_fasta_MSF:fasta_validation_wf:input_suffix_check (1) [100%] 1 of 1 \u2714 [c7/d34016] process > identify_fasta_MSF:fasta_validation_wf:seqkit (1) [100%] 1 of 1 \u2714 [- ] process > identify_fasta_MSF:virsorter_wf:virsorter [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:virsorter_wf:filter_virsorter - [- ] process > identify_fasta_MSF:virsorter_wf:virsorter_collect_data - [ff/f698ec] process > identify_fasta_MSF:virsorter2_wf:virsorter2 (1) [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:virsorter2_wf:filter_virsorter2 - [- ] process > identify_fasta_MSF:virsorter2_wf:virsorter2_collect_data - * * * [fc/e0fcc7] process > identify_fasta_MSF:pprmeta_wf:pprmeta (1) [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:pprmeta_wf:filter_PPRmeta - [- ] process > identify_fasta_MSF:pprmeta_wf:pprmeta_collect_data - [- ] process > identify_fasta_MSF:vibrant_wf:vibrant [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:vibrant_wf:filter_vibrant - [- ] process > identify_fasta_MSF:vibrant_wf:vibrant_collect_data - [- ] process > identify_fasta_MSF:vibrant_virome_wf:vibrant_virome [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:vibrant_virome_wf:filter_vibrant_virome - [- ] process > identify_fasta_MSF:vibrant_virome_wf:vibrant_virome_collect_data - [08/50ce71] process > identify_fasta_MSF:virnet_wf:normalize_contig_size (1) [100%] 1 of 1 \u2714 [- ] process > identify_fasta_MSF:virnet_wf:virnet [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:virnet_wf:filter_virnet - [- ] process > identify_fasta_MSF:virnet_wf:virnet_collect_data - [4e/ca5a07] process > identify_fasta_MSF:phigaro_wf:phigaro (1) [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:phigaro_wf:phigaro_collect_data - [- ] process > identify_fasta_MSF:seeker_wf:seeker [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:seeker_wf:filter_seeker - [- ] process > identify_fasta_MSF:seeker_wf:seeker_collect_data - [- ] process > identify_fasta_MSF:filter_tool_names - [- ] process > identify_fasta_MSF:r_plot - [- ] process > identify_fasta_MSF:upsetr_plot - [- ] process > identify_fasta_MSF:samtools - [- ] process > phage_annotation_MSF:prodigal - [- ] process > phage_annotation_MSF:hmmscan - [- ] process > phage_annotation_MSF:chromomap_parser - [- ] process > phage_annotation_MSF:chromomap - [- ] process > checkV_wf:checkV - [- ] process > phage_tax_classification:split_multi_fasta - [- ] process > phage_tax_classification:sourmash_for_tax -","title":"Let's do a Testrun"},{"location":"Workflow-execution/processes.html#started-process","text":"[4e/ca5a07] process > identify_fasta_MSF:phigaro_wf:phigaro (1) [ 0%] 0 of 1 [4e/ca5a07] : is the process folder where the tasks of this process is started/executed identify_fasta_MSF : is one of two sub workflows (identify sequence; phage annotation) phigaro_wf: : the Phigaro identification process contains a smaller workflow with phigaro and phigaro_collect_data (raw output of Phigaro) phigaro : is the actual phage identification task [ 0%] 0 of 1 : Process has not finished yet","title":"Started process"},{"location":"Workflow-execution/processes.html#finished-process","text":"this is how a finished process will look like this: [6b/79564a] process > identify_fasta_MSF:fasta_validation_wf:input_suffix_check (1) [100%] 1 of 1 \u2714 [6b/79564a] : is the process folder where the tasks of this process (input_suffix_check) is executed you can access the folder: default: cd /tmp/nextflow-phage-$USER/6b/79564a and tabcompletion in our case with --work-dir : cd work/6b/79564a and tab completion you will see the files used/created in this process identify_fasta_MSF : is one of two sub workflows (identify sequence; phage annotation) input_suffix_check (1) : is the actual task (in this case: checks if the fasta file has the correct format) [100%] 1 of 1 \u2714 : tells you that the process is complete","title":"Finished Process"},{"location":"Workflow-execution/processes.html#stored-process","text":"[skipped ] process > phage_blast_DB:phage_references_blastDB [100%] 1 of 1, stored: 1 \u2714 this is shown when you already e.g. downloaded the database so WtP won't execute this process again because the data is already stored","title":"Stored process"},{"location":"Workflow-execution/processes.html#process-in-queue","text":"[- ] process > identify_fasta_MSF:phigaro_wf:phigaro_collect_data - this is shows you that the process has not been started yet and is queued and waiting for execution","title":"Process in Queue"},{"location":"Workflow-execution/processes.html#oncomplete","text":"Done! Results are stored here --> results Thank you for using What the Phage Please cite us: https://doi.org/10.1101/2020.07.24.219899 Please also cite the other tools we use in our workflow --> results/literature Completed at: 30-Oct-2020 10:15:27 Duration : 49m 34s CPU hours : 5.2 Succeeded : 61 the onComplete message is shown when WtP is done here you find again some basic information e.g. how long was your run, where are the results and a literature-file for citing us and the tools WtP uses","title":"OnComplete"},{"location":"Workflow-execution/results.html","text":"Example run We executed the following commands do perform a WtP test-run from the Terminal-section: nextflow run phage.nf -profile smalltest,local,docker --work-dir --cores 16 WtP will create a results -folder in your current working dir (where you executed WtP) and a subfolder with the name of your input-fasta - in our case all_pos_phage Results literature contains a Citations.bib file which you can import the in your citation program to have all the citations of the programs WtP uses for its analysis run info execution_report.html: this report gives an overview over: distribution of resource usage for each process (CPU, Memory, Job duration) information about each task in the workflow execution_timeline.html: this report gives you an overview over the processes execution timeline all_pos_phage your actual results Test sample result annotation_results output we need for the chromomap identified_contigs_by_tools contains toolname.txt files show the sequence-name(fasta-header) identified by the corresponding tool phage_positive_contigs contains a multi-fasta-file with all the positive fasta-sequences identified by the tools you can use for further analysis raw_data contains the raw output from the phage-identification-tools packed as toolname.tar.gz -file upsetr.svg This chart (UpSetR plot) quantifies the result-intersections of the phage identification tools, similar to a Venn diagram. The amount of positive phage-sequences identified by each tool is represented on the left barplot in blue. The dot plot shows via line connection(s) which of the tools identified the exact same positive phage sequences. The amount of these shared matches is quantified as a barplot above each corresponding dot pattern. sample_overview-small.html, sample_overview-large.html The graphical output of the annotation shows an overview of the individual loci of the predicted ORFs and the corresponding genes in the fasta sequences identified as phages. For a better visibility, we have chosen 4 categories: tail, capsid, baseplate, and other. This output can be used to verify the identified sequences (if the predicted sequences make sense or not). The annotation results are additionally plotted in an interactive HTML-file and are available as a file for further analysis. For an interactive chromamap you can check out the following link: chromomap results phage_distribution.pdf Heatmap for phage sequences visualising the tool agreements per phage positive contig. taxonomic_classification sourmash classification (all_pos_phage_tax-class.tsv) so you know the name of the phages contig prediction_value predicted_organism_name pos_phage_0 1 gi-1001940386-gb-KU522583.1-__Enterobacteria__phage__ECGD1,__complete__genome pos_phage_1 1 gi-1001941262-gb-KU647626.1-__Arthrobacter__phage__KellEzio,__complete__genome pos_phage_2 1 gi-1001941379-gb-KU647627.1-__Arthrobacter__phage__Kitkat,__complete__genome pos_phage_3 1 gi-1001941506-gb-KU647628.1-__Arthrobacter__phage__Mudcat,__complete__genome pos_phage_4 1 gi-1001941623-gb-KU647629.1-__Arthrobacter__phage__BarretLemon,__complete__genome pos_phage_5 1 gi-1001941806-gb-KU598975.1-__Staphylococcus__phage__CNPx,__complete__genome pos_phage_6 1 gi-1001941908-gb-KU595432.1-__Xanthomonas__phage__f20-Xaj,__complete__genome pos_phage_7 1 gi-1001941964-gb-KU595433.1-__Xanthomonas__phage__f30-Xaj,__complete__genome pos_phage_8 1 gi-1001942019-gb-KU595434.1-__Xanthomonas__phage__f29-Xaj,__complete__genome pos_phage_9 1 gi-1001942081-gb-KT624200.1-__Bacillus__phage__SP-15,__complete__genome all_pos_phage_quality_summary.tsv check CheckV for a detailed explanation contig_id contig_length genome_copies gene_count viral_genes host_genes checkv_quality miuvig_quality completeness completeness_method contamination provirus pos_phage_0 146647 1 243 141 1 High-quality High-quality 97.03 AAI-based 0 No pos_phage_1 58871 1 97 21 0 High-quality High-quality 100 AAI-based 0 No pos_phage_2 58560 1 95 20 0 High-quality High-quality 99.47 AAI-based 0 No pos_phage_3 59443 1 90 52 0 High-quality High-quality 100 AAI-based 0 No pos_phage_4 51290 1 74 44 0 High-quality High-quality 100 AAI-based 0 No pos_phage_5 43293 1 69 55 0 High-quality High-quality 100 AAI-based 0 No pos_phage_6 43851 1 53 30 0 High-quality High-quality 98.71 AAI-based 0 No pos_phage_7 44262 1 54 31 0 High-quality High-quality 99.64 AAI-based 0 No pos_phage_8 41865 1 60 57 0 High-quality High-quality 97.29 AAI-based 0 No pos_phage_9 221908 1 310 48 9 High-quality High-quality 100 AAI-based 0 No","title":"Results"},{"location":"Workflow-execution/results.html#example-run","text":"We executed the following commands do perform a WtP test-run from the Terminal-section: nextflow run phage.nf -profile smalltest,local,docker --work-dir --cores 16 WtP will create a results -folder in your current working dir (where you executed WtP) and a subfolder with the name of your input-fasta - in our case all_pos_phage","title":"Example run"},{"location":"Workflow-execution/results.html#results","text":"literature contains a Citations.bib file which you can import the in your citation program to have all the citations of the programs WtP uses for its analysis run info execution_report.html: this report gives an overview over: distribution of resource usage for each process (CPU, Memory, Job duration) information about each task in the workflow execution_timeline.html: this report gives you an overview over the processes execution timeline all_pos_phage your actual results","title":"Results"},{"location":"Workflow-execution/results.html#test-sample-result","text":"annotation_results output we need for the chromomap identified_contigs_by_tools contains toolname.txt files show the sequence-name(fasta-header) identified by the corresponding tool phage_positive_contigs contains a multi-fasta-file with all the positive fasta-sequences identified by the tools you can use for further analysis raw_data contains the raw output from the phage-identification-tools packed as toolname.tar.gz -file upsetr.svg This chart (UpSetR plot) quantifies the result-intersections of the phage identification tools, similar to a Venn diagram. The amount of positive phage-sequences identified by each tool is represented on the left barplot in blue. The dot plot shows via line connection(s) which of the tools identified the exact same positive phage sequences. The amount of these shared matches is quantified as a barplot above each corresponding dot pattern. sample_overview-small.html, sample_overview-large.html The graphical output of the annotation shows an overview of the individual loci of the predicted ORFs and the corresponding genes in the fasta sequences identified as phages. For a better visibility, we have chosen 4 categories: tail, capsid, baseplate, and other. This output can be used to verify the identified sequences (if the predicted sequences make sense or not). The annotation results are additionally plotted in an interactive HTML-file and are available as a file for further analysis. For an interactive chromamap you can check out the following link: chromomap results phage_distribution.pdf Heatmap for phage sequences visualising the tool agreements per phage positive contig. taxonomic_classification sourmash classification (all_pos_phage_tax-class.tsv) so you know the name of the phages contig prediction_value predicted_organism_name pos_phage_0 1 gi-1001940386-gb-KU522583.1-__Enterobacteria__phage__ECGD1,__complete__genome pos_phage_1 1 gi-1001941262-gb-KU647626.1-__Arthrobacter__phage__KellEzio,__complete__genome pos_phage_2 1 gi-1001941379-gb-KU647627.1-__Arthrobacter__phage__Kitkat,__complete__genome pos_phage_3 1 gi-1001941506-gb-KU647628.1-__Arthrobacter__phage__Mudcat,__complete__genome pos_phage_4 1 gi-1001941623-gb-KU647629.1-__Arthrobacter__phage__BarretLemon,__complete__genome pos_phage_5 1 gi-1001941806-gb-KU598975.1-__Staphylococcus__phage__CNPx,__complete__genome pos_phage_6 1 gi-1001941908-gb-KU595432.1-__Xanthomonas__phage__f20-Xaj,__complete__genome pos_phage_7 1 gi-1001941964-gb-KU595433.1-__Xanthomonas__phage__f30-Xaj,__complete__genome pos_phage_8 1 gi-1001942019-gb-KU595434.1-__Xanthomonas__phage__f29-Xaj,__complete__genome pos_phage_9 1 gi-1001942081-gb-KT624200.1-__Bacillus__phage__SP-15,__complete__genome all_pos_phage_quality_summary.tsv check CheckV for a detailed explanation contig_id contig_length genome_copies gene_count viral_genes host_genes checkv_quality miuvig_quality completeness completeness_method contamination provirus pos_phage_0 146647 1 243 141 1 High-quality High-quality 97.03 AAI-based 0 No pos_phage_1 58871 1 97 21 0 High-quality High-quality 100 AAI-based 0 No pos_phage_2 58560 1 95 20 0 High-quality High-quality 99.47 AAI-based 0 No pos_phage_3 59443 1 90 52 0 High-quality High-quality 100 AAI-based 0 No pos_phage_4 51290 1 74 44 0 High-quality High-quality 100 AAI-based 0 No pos_phage_5 43293 1 69 55 0 High-quality High-quality 100 AAI-based 0 No pos_phage_6 43851 1 53 30 0 High-quality High-quality 98.71 AAI-based 0 No pos_phage_7 44262 1 54 31 0 High-quality High-quality 99.64 AAI-based 0 No pos_phage_8 41865 1 60 57 0 High-quality High-quality 97.29 AAI-based 0 No pos_phage_9 221908 1 310 48 9 High-quality High-quality 100 AAI-based 0 No","title":"Test sample result"},{"location":"installation/nextflow.html","text":"Install Java runtime and Nextflow Nextflow is quite easy to install: install a java run time for Nextflow if not available sudo apt-get update sudo apt install -y default-jre install nextflow : # download curl -s https://get.nextflow.io | bash # move to any $PATH location or add it to $PATH # e.g. sudo mv nextflow /usr/bin/ Update Nextflow to a newer version just execute this: alternatively use the \"build in\" nextflow upgrade # downloads the newer version curl -s https://get.nextflow.io | bash # replace the old version with the newer version sudo mv nextflow /usr/bin/ Specific Nextflow versions if you need a specific older nextflow version for some reason, go to the release page under assets you find a file called \"nextflow-RELEASENUMBER-all\" (replace RELEASENUMBER with the actual number they provide on the release page) download this file and move it to e.g. /usr/bin/ or call it directly ./nextflow-RELEASENUMBER-all run ... instead of nextflow run ... you do nextflow-RELEASENUMBER-all run ...","title":"Nextflow"},{"location":"installation/nextflow.html#install-java-runtime-and-nextflow","text":"Nextflow is quite easy to install: install a java run time for Nextflow if not available sudo apt-get update sudo apt install -y default-jre install nextflow : # download curl -s https://get.nextflow.io | bash # move to any $PATH location or add it to $PATH # e.g. sudo mv nextflow /usr/bin/","title":"Install Java runtime and Nextflow"},{"location":"installation/nextflow.html#update-nextflow-to-a-newer-version","text":"just execute this: alternatively use the \"build in\" nextflow upgrade # downloads the newer version curl -s https://get.nextflow.io | bash # replace the old version with the newer version sudo mv nextflow /usr/bin/","title":"Update Nextflow to a newer version"},{"location":"installation/nextflow.html#specific-nextflow-versions","text":"if you need a specific older nextflow version for some reason, go to the release page under assets you find a file called \"nextflow-RELEASENUMBER-all\" (replace RELEASENUMBER with the actual number they provide on the release page) download this file and move it to e.g. /usr/bin/ or call it directly ./nextflow-RELEASENUMBER-all run ... instead of nextflow run ... you do nextflow-RELEASENUMBER-all run ...","title":"Specific Nextflow versions"},{"location":"installation/quick_installation.html","text":"Local, docker usage \"None informaticians / newcomer to bioinformatics\" approach using ubuntu [admin rights required] we do not recommend this installation-process (an older version of Docker will be installed. This installation method is not suitable for a cluster), but it is probably the fastest way to get WtP to run sudo apt-get update sudo apt install -y default-jre curl -s https://get.nextflow.io | bash sudo mv nextflow /usr/bin/ sudo apt-get install -y docker-ce docker-ce-cli containerd.io sudo usermod -a -G docker $USER Restart your computer Conda usage: conda create -n wtp nextflow==20.07.01 singularity==3.6.1 conda activate wtp (wtp) $ nextflow run replikation/What_the_Phage -r v1.0.1 --setup ... now you should be able to run nextflow run replikation/What_the_Phage -r v1.0.1 --setup -profile local,singularity Test the workflow for docker (local use) nextflow run replikation/What_the_Phage -r v1.0.1 --cores 8 -profile smalltest,local,docker for singularity (slurm use) nextflow run replikation/What_the_Phage -r v1.0.1 --cores 8 -profile smalltest,slurm,singularity","title":"Quick Installation"},{"location":"installation/quick_installation.html#local-docker-usage","text":"\"None informaticians / newcomer to bioinformatics\" approach using ubuntu [admin rights required] we do not recommend this installation-process (an older version of Docker will be installed. This installation method is not suitable for a cluster), but it is probably the fastest way to get WtP to run sudo apt-get update sudo apt install -y default-jre curl -s https://get.nextflow.io | bash sudo mv nextflow /usr/bin/ sudo apt-get install -y docker-ce docker-ce-cli containerd.io sudo usermod -a -G docker $USER Restart your computer","title":"Local, docker usage"},{"location":"installation/quick_installation.html#conda-usage","text":"conda create -n wtp nextflow==20.07.01 singularity==3.6.1 conda activate wtp (wtp) $ nextflow run replikation/What_the_Phage -r v1.0.1 --setup ... now you should be able to run nextflow run replikation/What_the_Phage -r v1.0.1 --setup -profile local,singularity","title":"Conda usage:"},{"location":"installation/quick_installation.html#test-the-workflow","text":"for docker (local use) nextflow run replikation/What_the_Phage -r v1.0.1 --cores 8 -profile smalltest,local,docker for singularity (slurm use) nextflow run replikation/What_the_Phage -r v1.0.1 --cores 8 -profile smalltest,slurm,singularity","title":"Test the workflow"},{"location":"installation/Engine/docker.html","text":"Docker installation to install the most recent version we recommend the following Docker installation they also describe how to install docker on debian, or other distros in the following section we describe how to install docker on ubuntu For Ubuntu we are using this installation guide but shorten it to the bare minimum for you so we skip a few steps that are not necessary Setting up the repository Before you install Docker Engine for the first time on a new host machine, you need to set up the Docker repository. Afterward, you can install and update Docker from the repository (via apt update and apt upgrade ). sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common Add Docker\u2019s official GPG key: we skip the verification here curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Note: The lsb_release -cs sub-command below returns the name of your Ubuntu distribution, such as xenial. Sometimes, in a distribution like Linux Mint, you might need to change $(lsb_release -cs) to your parent Ubuntu distribution. For example, if you are using Linux Mint Tessa, you could use bionic. Docker does not offer any guarantees on untested and unsupported Ubuntu distributions. for x86_64 / amd64 sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" Install Docker Engine install via sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io Verify that Docker Engine is installed correctly by running the hello-world image. sudo docker run --rm hello-world This command downloads a test image and runs it in a container. When the container runs, it prints an informational message and exits. Docker Engine is installed and running. Create docker group docker needs to run without sudo for nextflow so we need to add user that want to use docker to the docker group Create the docker group (should be already created after the installation) sudo groupadd docker add a user to the docker group. sudo usermod -aG docker $USER Restart","title":"Docker"},{"location":"installation/Engine/docker.html#docker-installation","text":"to install the most recent version we recommend the following Docker installation they also describe how to install docker on debian, or other distros in the following section we describe how to install docker on ubuntu","title":"Docker installation"},{"location":"installation/Engine/docker.html#for-ubuntu","text":"we are using this installation guide but shorten it to the bare minimum for you so we skip a few steps that are not necessary","title":"For Ubuntu"},{"location":"installation/Engine/docker.html#setting-up-the-repository","text":"Before you install Docker Engine for the first time on a new host machine, you need to set up the Docker repository. Afterward, you can install and update Docker from the repository (via apt update and apt upgrade ). sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common Add Docker\u2019s official GPG key: we skip the verification here curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Note: The lsb_release -cs sub-command below returns the name of your Ubuntu distribution, such as xenial. Sometimes, in a distribution like Linux Mint, you might need to change $(lsb_release -cs) to your parent Ubuntu distribution. For example, if you are using Linux Mint Tessa, you could use bionic. Docker does not offer any guarantees on untested and unsupported Ubuntu distributions. for x86_64 / amd64 sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\"","title":"Setting up the repository"},{"location":"installation/Engine/docker.html#install-docker-engine","text":"install via sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io Verify that Docker Engine is installed correctly by running the hello-world image. sudo docker run --rm hello-world This command downloads a test image and runs it in a container. When the container runs, it prints an informational message and exits. Docker Engine is installed and running.","title":"Install Docker Engine"},{"location":"installation/Engine/docker.html#create-docker-group","text":"docker needs to run without sudo for nextflow so we need to add user that want to use docker to the docker group Create the docker group (should be already created after the installation) sudo groupadd docker add a user to the docker group. sudo usermod -aG docker $USER Restart","title":"Create docker group"},{"location":"installation/Engine/singularity.html","text":"Singularity installation we are using this installation guide For Ubuntu Install system dependencies You must first install development tools and libraries to your host. $ sudo apt-get update && \\ sudo apt-get install -y build-essential \\ libseccomp-dev pkg-config squashfs-tools cryptsetup Install Golang This is one of several ways to install and configure golang. First, download the Golang archive to /tmp, then extract the archive to /usr/local. NOTE: if you are updating Go from an older version, make sure you remove /usr/local/go before reinstalling it. export VERSION=1.14.9 OS=linux ARCH=amd64 # change this as you need wget -O /tmp/go${VERSION}.${OS}-${ARCH}.tar.gz https://dl.google.com/go/go${VERSION}.${OS}-${ARCH}.tar.gz && \\ sudo tar -C /usr/local -xzf /tmp/go${VERSION}.${OS}-${ARCH}.tar.gz Finally, set up your environment for Go: echo 'export GOPATH=${HOME}/go' >> ~/.bashrc && \\ echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' >> ~/.bashrc && \\ source ~/.bashrc Install golangci-lint In order to install golangci-lint, you can run: curl -sfL https://install.goreleaser.com/github.com/golangci/golangci-lint.sh | sh -s -- -b $(go env GOPATH)/bin v1.21.0 This will download and install golangci-lint from its Github releases page (using version v1.21.0 at the moment). Clone the repo Golang is a bit finicky about where things are placed. Here is the correct way to build Singularity from source: mkdir -p ${GOPATH}/src/github.com/sylabs && \\ cd ${GOPATH}/src/github.com/sylabs && \\ git clone https://github.com/sylabs/singularity.git && \\ cd singularity To build a stable version of Singularity, check out a release tag before compiling: git checkout v3.6.3 Compiling Singularity You can build Singularity using the following commands: cd ${GOPATH}/src/github.com/sylabs/singularity && \\ ./mconfig && \\ cd ./builddir && \\ make && \\ sudo make install And that's it! Now you can check your Singularity version by running: singularity version To build in a different folder and to set the install prefix to a different path: ./mconfig -b ./buildtree -p /usr/local","title":"Singularity"},{"location":"installation/Engine/singularity.html#singularity-installation","text":"we are using this installation guide","title":"Singularity installation"},{"location":"installation/Engine/singularity.html#for-ubuntu","text":"Install system dependencies You must first install development tools and libraries to your host. $ sudo apt-get update && \\ sudo apt-get install -y build-essential \\ libseccomp-dev pkg-config squashfs-tools cryptsetup","title":"For Ubuntu"},{"location":"installation/Engine/singularity.html#install-golang","text":"This is one of several ways to install and configure golang. First, download the Golang archive to /tmp, then extract the archive to /usr/local. NOTE: if you are updating Go from an older version, make sure you remove /usr/local/go before reinstalling it. export VERSION=1.14.9 OS=linux ARCH=amd64 # change this as you need wget -O /tmp/go${VERSION}.${OS}-${ARCH}.tar.gz https://dl.google.com/go/go${VERSION}.${OS}-${ARCH}.tar.gz && \\ sudo tar -C /usr/local -xzf /tmp/go${VERSION}.${OS}-${ARCH}.tar.gz Finally, set up your environment for Go: echo 'export GOPATH=${HOME}/go' >> ~/.bashrc && \\ echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' >> ~/.bashrc && \\ source ~/.bashrc","title":"Install Golang"},{"location":"installation/Engine/singularity.html#install-golangci-lint","text":"In order to install golangci-lint, you can run: curl -sfL https://install.goreleaser.com/github.com/golangci/golangci-lint.sh | sh -s -- -b $(go env GOPATH)/bin v1.21.0 This will download and install golangci-lint from its Github releases page (using version v1.21.0 at the moment).","title":"Install golangci-lint"},{"location":"installation/Engine/singularity.html#clone-the-repo","text":"Golang is a bit finicky about where things are placed. Here is the correct way to build Singularity from source: mkdir -p ${GOPATH}/src/github.com/sylabs && \\ cd ${GOPATH}/src/github.com/sylabs && \\ git clone https://github.com/sylabs/singularity.git && \\ cd singularity To build a stable version of Singularity, check out a release tag before compiling: git checkout v3.6.3","title":"Clone the repo"},{"location":"installation/Engine/singularity.html#compiling-singularity","text":"You can build Singularity using the following commands: cd ${GOPATH}/src/github.com/sylabs/singularity && \\ ./mconfig && \\ cd ./builddir && \\ make && \\ sudo make install And that's it! Now you can check your Singularity version by running: singularity version To build in a different folder and to set the install prefix to a different path: ./mconfig -b ./buildtree -p /usr/local","title":"Compiling Singularity"},{"location":"installation/Environment/environment.html","text":"WtP can be executed in different environments (workload management platforms) Configurations Work in progress coming soon ;)","title":"Local/Cluster"},{"location":"installation/Environment/environment.html#configurations","text":"Work in progress coming soon ;)","title":"Configurations"}]}